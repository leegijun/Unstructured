{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 번호를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emphasized': 13944,\n",
       " 'smelly': 23068,\n",
       " 'grisby': 13401,\n",
       " 'undated': 54008,\n",
       " 'villard': 47250,\n",
       " \"harlin's\": 15889,\n",
       " 'morbidly': 21900,\n",
       " \"developer's\": 45294,\n",
       " \"thomas's\": 78545,\n",
       " 'vieques': 61454,\n",
       " 'wrought': 12013,\n",
       " 'hutu': 70268,\n",
       " 'baurki': 53989,\n",
       " 'zoloft': 68779,\n",
       " 'todays': 8841,\n",
       " 'fruttis': 62927,\n",
       " 'houghton': 42027,\n",
       " 'whiffs': 42098,\n",
       " 'daughters': 2844,\n",
       " 'kum': 35287,\n",
       " 'promulgated': 81356,\n",
       " 'elkaim': 28392,\n",
       " 'arent': 38048,\n",
       " 'enlisting': 23822,\n",
       " \"mordred's\": 87901,\n",
       " 'guiness': 11320,\n",
       " 'music\\x96the': 58429,\n",
       " 'lineal': 86225,\n",
       " \"'crunching'\": 81066,\n",
       " 'advisement': 60952,\n",
       " 'shortens': 36594,\n",
       " 'blabbing': 46559,\n",
       " 'pinhead': 32478,\n",
       " 'laughting': 51634,\n",
       " 'slugger': 35742,\n",
       " 'smooch': 46943,\n",
       " 'shoenumber': 79245,\n",
       " 'hickory': 21929,\n",
       " \"welles's\": 50114,\n",
       " 'easyrider': 65403,\n",
       " 'disillusion': 28451,\n",
       " \"'government\": 86216,\n",
       " 'hitman': 20958,\n",
       " 'fantasizes': 36979,\n",
       " 'mcelhone': 24712,\n",
       " 'baywatch': 17910,\n",
       " \"dreamin'\": 63758,\n",
       " 'misused': 16577,\n",
       " 'illogical': 4327,\n",
       " \"being's\": 73780,\n",
       " 'overfed': 54903,\n",
       " \"'stomp\": 77031,\n",
       " 'chan': 2832,\n",
       " 'difranco': 83007,\n",
       " \"mansion's\": 28892,\n",
       " 'hobart': 28255,\n",
       " \"expectations'\": 38823,\n",
       " 'turf': 17320,\n",
       " 'steinitz': 50679,\n",
       " 'tyrone': 12907,\n",
       " 'rejoice': 18206,\n",
       " 'involvements': 45916,\n",
       " 'madona': 73845,\n",
       " 'completeist': 58383,\n",
       " 'talks': 2323,\n",
       " 'signage': 63568,\n",
       " \"dancers'\": 32273,\n",
       " 'oxford': 18490,\n",
       " 'rememberances': 60011,\n",
       " 'tyrannosaur': 62140,\n",
       " \"'90s\": 14327,\n",
       " 'tkom': 59483,\n",
       " \"bat'\": 38367,\n",
       " 'guillame': 36048,\n",
       " 'encrusted': 41561,\n",
       " 'homosexual': 4714,\n",
       " \"'everybody's\": 87465,\n",
       " 'vehemently': 26234,\n",
       " '22nd': 50821,\n",
       " \"'acting'\": 24609,\n",
       " \"much'\": 33152,\n",
       " 'gordons': 56426,\n",
       " 'chocolates': 24546,\n",
       " 'uppers': 49485,\n",
       " 'mclachlan': 72302,\n",
       " 'pornichet': 45880,\n",
       " 'photogrsphed': 69317,\n",
       " 'scattered': 8810,\n",
       " 'deducted': 48765,\n",
       " 'zzzzz': 84867,\n",
       " 'boobs': 8252,\n",
       " 'items': 5275,\n",
       " 'devours': 28221,\n",
       " 'fifteenth': 41396,\n",
       " \"'modern'\": 34633,\n",
       " 'ensembles': 34775,\n",
       " 'smmf': 31438,\n",
       " 'enliven': 24296,\n",
       " 'noble': 3140,\n",
       " 'thrillingly': 51875,\n",
       " \"'henry\": 27557,\n",
       " 'lifeboats': 27919,\n",
       " 'sulking': 28377,\n",
       " \"'won'\": 75660,\n",
       " 'corinthians': 42318,\n",
       " 'baily': 47476,\n",
       " 'maclhuen': 56014,\n",
       " 'fazes': 63573,\n",
       " 'odete': 43859,\n",
       " 'polity': 87945,\n",
       " 'ohohh': 81680,\n",
       " \"han's\": 46063,\n",
       " '\\x84richard': 62236,\n",
       " 'paycheque': 57816,\n",
       " 'tottered': 63809,\n",
       " 'boffing': 45425,\n",
       " 'lady': 758,\n",
       " 'promotion': 9588,\n",
       " \"d'etre'\": 73307,\n",
       " 'newscaster': 33471,\n",
       " \"munnera'la\": 84592,\n",
       " 'pronunciation': 23332,\n",
       " 'ultraboring': 53587,\n",
       " 'riffed': 45368,\n",
       " 'composer': 5099,\n",
       " 'phineas': 73398,\n",
       " 'padme': 50513,\n",
       " 'tensions': 9480,\n",
       " 'thoroughfare': 88544,\n",
       " 'fastballs': 68080,\n",
       " 'rods': 34841,\n",
       " 'unremittingly': 29810,\n",
       " 'fjernsynsteatret': 75000,\n",
       " 'zaping': 82250,\n",
       " \"male's\": 46809,\n",
       " 'finger': 4385,\n",
       " 'farnworth': 85883,\n",
       " 'unmediated': 46138,\n",
       " 'mencia': 15639,\n",
       " \"je'taime's\": 73412,\n",
       " 'laughable': 1319,\n",
       " \"vampiras'\": 77126,\n",
       " \"shepard's\": 25405,\n",
       " \"'moral'\": 40662,\n",
       " 'consultant': 14778,\n",
       " 'goosebump': 60726,\n",
       " 'filmability': 54062,\n",
       " 'highlanders': 55080,\n",
       " 'relations': 4246,\n",
       " 'briant': 63270,\n",
       " 'shout': 8517,\n",
       " 'duffell': 17165,\n",
       " 'helming': 25635,\n",
       " 'declaims': 44624,\n",
       " 'reissue': 32990,\n",
       " 'ossuary': 69386,\n",
       " 'otherwise': 897,\n",
       " 'preacher': 8821,\n",
       " 'avalanches': 84560,\n",
       " 'pregnant\\x97what': 57984,\n",
       " 'astrotech': 81171,\n",
       " 'ullswater': 79419,\n",
       " \"pappy's\": 69630,\n",
       " 'location': 1619,\n",
       " \"sweat'\": 34837,\n",
       " 'zomg': 87798,\n",
       " 'critically': 10473,\n",
       " 'loathesome': 80323,\n",
       " 'paucity': 31169,\n",
       " \"'coast\": 74521,\n",
       " 'vivir': 82880,\n",
       " 'melts': 13526,\n",
       " 'unopened': 86613,\n",
       " 'advancements': 28668,\n",
       " \"lowe's\": 32786,\n",
       " 'deeling': 68502,\n",
       " \"style'\": 34623,\n",
       " 'wrestle': 18716,\n",
       " 'goldstein': 45539,\n",
       " 'viennese': 19481,\n",
       " 'tester': 27893,\n",
       " 'raffin': 27584,\n",
       " 'sinecures': 62468,\n",
       " 'object': 3255,\n",
       " 'pauley': 53328,\n",
       " 'milligans': 66333,\n",
       " 'pup': 18684,\n",
       " 'gilmore': 25553,\n",
       " 'zone': 3251,\n",
       " 'carbide': 49217,\n",
       " 'hexagonal': 60384,\n",
       " 'penciled': 57360,\n",
       " 'easterners': 81651,\n",
       " 'brood': 25112,\n",
       " '1940': 5881,\n",
       " 'wallah': 26979,\n",
       " 'euthanasiarist': 79696,\n",
       " 'gital': 79522,\n",
       " 'dropouts': 79546,\n",
       " 'burkhardt': 60713,\n",
       " 'opposing': 10176,\n",
       " 'lakeridge': 81574,\n",
       " \"'brella\": 55938,\n",
       " \"troy's\": 64500,\n",
       " 'precedes': 17738,\n",
       " 'noisy': 11870,\n",
       " 'rebuttal': 44347,\n",
       " 'engages': 11959,\n",
       " '“sanatorium”': 64509,\n",
       " 'lynley': 84534,\n",
       " 'focalize': 74247,\n",
       " 'coyly': 47333,\n",
       " 'scorn': 23029,\n",
       " 'fangirls': 44049,\n",
       " 'kathmandu': 23698,\n",
       " \"'why'\": 49489,\n",
       " 'anan': 71994,\n",
       " \"zealander's\": 77979,\n",
       " 'narrowly': 16539,\n",
       " 'obliterated': 23992,\n",
       " \"yankee's\": 46418,\n",
       " 'luján': 25822,\n",
       " \"meyjes'\": 65789,\n",
       " 'hungrily': 31718,\n",
       " 'commiserate': 47311,\n",
       " 'unexceptional': 29295,\n",
       " 'immortel': 33541,\n",
       " 'samedi': 32038,\n",
       " \"woolrich's\": 81493,\n",
       " 'notable': 2875,\n",
       " 'headset': 46525,\n",
       " 'uno': 81528,\n",
       " 'fillum': 71597,\n",
       " 'crackles': 32700,\n",
       " 'ayurvedic': 38218,\n",
       " 'honour': 9655,\n",
       " \"rosario's\": 37281,\n",
       " 'appeased': 50434,\n",
       " 'bubbling': 18799,\n",
       " 'jetski': 60823,\n",
       " 'soetman': 67929,\n",
       " 'secretions': 39042,\n",
       " 'took': 559,\n",
       " 'compulsive': 11009,\n",
       " 'supercharged': 59593,\n",
       " \"prostitute's\": 35414,\n",
       " 'chun': 39681,\n",
       " 'clampets': 72165,\n",
       " \"herzog's\": 37627,\n",
       " 'sunnydale': 61460,\n",
       " 'course': 262,\n",
       " 'snoozefest': 31298,\n",
       " 'madonna': 4638,\n",
       " 'motorist': 22215,\n",
       " \"jack's\": 8144,\n",
       " 'cement': 11809,\n",
       " 'adept': 10875,\n",
       " 'creaks': 24180,\n",
       " 'tsotg': 82254,\n",
       " 'knicker': 44983,\n",
       " 'showgirls': 11608,\n",
       " 'schreck': 68118,\n",
       " 'relecting': 53470,\n",
       " 'staginess': 27057,\n",
       " 'rag': 15783,\n",
       " 'kiosks': 62605,\n",
       " 'spiel': 26310,\n",
       " 'float': 10213,\n",
       " 'luckett': 36075,\n",
       " 'tremain': 73410,\n",
       " 'manhunt': 22429,\n",
       " 'fiving': 47975,\n",
       " 'pajamas': 27374,\n",
       " 'drabness': 84398,\n",
       " 'puppet': 5073,\n",
       " 'homely': 14864,\n",
       " 'personnal': 72749,\n",
       " 'assination': 54995,\n",
       " 'nits': 38345,\n",
       " 'wirth': 28812,\n",
       " \"indonesia'\": 52759,\n",
       " 'meh': 26147,\n",
       " 'urmila': 15512,\n",
       " 'pretend': 3942,\n",
       " 'death': 338,\n",
       " 'prudishness': 79894,\n",
       " \"surrender's\": 69190,\n",
       " 'designing': 40838,\n",
       " 'intensified': 29368,\n",
       " 'schirripa': 60654,\n",
       " 'undermined': 13354,\n",
       " 'boyars': 83474,\n",
       " 'musique': 50962,\n",
       " 'gigantic': 7339,\n",
       " \"eibon'\": 81093,\n",
       " \"'begin'\": 78142,\n",
       " 'divorcee': 20896,\n",
       " 'keoma': 57709,\n",
       " 'parole': 11552,\n",
       " 'discharge': 32004,\n",
       " 'appreciably': 45947,\n",
       " 'tellegen': 60588,\n",
       " 'enquanto': 62700,\n",
       " 'rarefied': 23582,\n",
       " 'arshad': 69294,\n",
       " 'frets': 45938,\n",
       " 'underestimating': 35451,\n",
       " 'prompting': 23257,\n",
       " 'louisville': 45004,\n",
       " 'satisfied': 4092,\n",
       " 'contagious': 23984,\n",
       " 'unlikely': 2385,\n",
       " 'rageddy': 86667,\n",
       " 'maura': 76880,\n",
       " '1930s': 3811,\n",
       " 'lightheartedly': 45754,\n",
       " 'coffey': 54331,\n",
       " 'groupthink': 79836,\n",
       " 'enid': 39113,\n",
       " 'uniform': 6333,\n",
       " 'ebon': 60237,\n",
       " \"clockwatchers'\": 61675,\n",
       " \"'tales'\": 85524,\n",
       " 'untypical': 36434,\n",
       " 'pillsbury': 67991,\n",
       " 'terminators': 62344,\n",
       " 'athletic': 8740,\n",
       " 'marylee': 13479,\n",
       " 'barabas': 46957,\n",
       " 'engage': 4523,\n",
       " 'horsies': 54131,\n",
       " 'costanzo': 80594,\n",
       " \"it'good\": 52469,\n",
       " 'excitedly': 36812,\n",
       " 'donnybrook': 46065,\n",
       " 'epileptic': 30229,\n",
       " 'louvers': 57622,\n",
       " \"'things\": 30436,\n",
       " 'tubby': 69451,\n",
       " \"umetsu's\": 56176,\n",
       " \"cow's\": 84612,\n",
       " 'ejaculating': 62322,\n",
       " 'sentinela': 79473,\n",
       " 'biplane': 38970,\n",
       " 'discomfort': 15480,\n",
       " 'trounce': 50671,\n",
       " 'afi': 14500,\n",
       " \"modesty'\": 53723,\n",
       " 'materialises': 35796,\n",
       " 'basing': 16754,\n",
       " 'beneath': 4155,\n",
       " 'kuba': 54637,\n",
       " 'flimsiest': 27923,\n",
       " 'flavin': 48676,\n",
       " 'bolted': 32263,\n",
       " 'chunks': 15919,\n",
       " 'directorship': 49290,\n",
       " \"neeson's\": 86247,\n",
       " 'salmonova': 65966,\n",
       " 'betrail': 85273,\n",
       " 'belieavablitly': 56395,\n",
       " 'darden': 46396,\n",
       " 'joyfully': 28008,\n",
       " 'wickes': 29269,\n",
       " 'vapours': 79471,\n",
       " \"'cool\": 28121,\n",
       " 'program': 2078,\n",
       " 'baloer': 84452,\n",
       " 'rummaged': 65622,\n",
       " \"marcus's\": 82098,\n",
       " 'knuckler': 47438,\n",
       " 'equivalencing': 76094,\n",
       " 'suet': 35309,\n",
       " \"wern't\": 52108,\n",
       " 'hollywoodized': 23041,\n",
       " 'spielbergian': 45664,\n",
       " 'plainer': 76142,\n",
       " 'auto': 7092,\n",
       " \"hood's\": 27211,\n",
       " 'cosmetic': 24117,\n",
       " 'coyotes': 84109,\n",
       " 'lerman': 38686,\n",
       " 'agbayani': 58353,\n",
       " 'chirin': 38848,\n",
       " 'vette': 33938,\n",
       " 'mannequins': 23558,\n",
       " 'oneiros': 66927,\n",
       " 'angelwas': 53185,\n",
       " 'attached': 3461,\n",
       " 'ethnically': 64146,\n",
       " 'passivity': 28506,\n",
       " 'doe': 12570,\n",
       " 'lifeboat': 19109,\n",
       " \"quality'\": 87926,\n",
       " 'oracular': 73167,\n",
       " 'allan': 8510,\n",
       " 'handbags': 52315,\n",
       " 'those': 145,\n",
       " 'kojak': 26445,\n",
       " \"'really'\": 42638,\n",
       " 'unpalatable': 28046,\n",
       " 'umms': 74446,\n",
       " 'villian': 19454,\n",
       " 'tendencies': 12522,\n",
       " 'khomeini': 69785,\n",
       " \"val's\": 40253,\n",
       " 'gandus': 70116,\n",
       " 'skeptic': 22552,\n",
       " 'inventive': 4409,\n",
       " 'piss': 76505,\n",
       " 'egyptologists': 59596,\n",
       " 'vaporised': 55884,\n",
       " 'tumors': 42353,\n",
       " \"rock'em\": 87873,\n",
       " 'verucci': 83461,\n",
       " \"craven's\": 9293,\n",
       " 'estrella': 73196,\n",
       " 'prival': 87057,\n",
       " \"school's\": 12936,\n",
       " \"renny's\": 53692,\n",
       " 'risking': 17107,\n",
       " 'sarro': 35463,\n",
       " 'sim': 17939,\n",
       " 'softcore': 11673,\n",
       " 'kneecaps': 47730,\n",
       " \"italian's\": 85420,\n",
       " 'dragged': 3314,\n",
       " 'pool': 3070,\n",
       " 'rounders': 22753,\n",
       " 'ginuea': 63335,\n",
       " 'burbridge': 44139,\n",
       " 'duddley': 67013,\n",
       " 'balling': 33732,\n",
       " 'beginners': 20347,\n",
       " 'clumped': 41939,\n",
       " 'rachels': 51852,\n",
       " 'cohens': 67454,\n",
       " 'alecia': 29994,\n",
       " \"10'x10'\": 82037,\n",
       " 'burger': 17181,\n",
       " 'deb': 39009,\n",
       " 'schooler': 17928,\n",
       " 'brig': 79711,\n",
       " \"philips'\": 71691,\n",
       " \"nerds'\": 40885,\n",
       " \"'assa'\": 57139,\n",
       " 'bloat': 41434,\n",
       " 'priuses': 52475,\n",
       " 'klasky': 50627,\n",
       " 'manifesto': 30867,\n",
       " 'melora': 84370,\n",
       " \"library's\": 61080,\n",
       " 'martinez': 13521,\n",
       " 'corben': 55520,\n",
       " \"'take\": 40409,\n",
       " 'chutki': 65554,\n",
       " 'motorboat': 39665,\n",
       " 'overdoes': 18063,\n",
       " 'morton': 11654,\n",
       " 'holloway': 16917,\n",
       " \"townspeople's\": 45843,\n",
       " 'uncertainties': 34833,\n",
       " 'godmother': 11779,\n",
       " 'permissions': 73949,\n",
       " 'charachter': 41163,\n",
       " 'kornbluths': 79268,\n",
       " \"devito's\": 41104,\n",
       " \"shaquille's\": 86877,\n",
       " 'modicum': 12245,\n",
       " 'destinies': 21764,\n",
       " 'survive': 2004,\n",
       " 'wounder': 45436,\n",
       " 'sachin': 68011,\n",
       " 'uproariously': 22185,\n",
       " 'sporca': 78884,\n",
       " \"'her'\": 37811,\n",
       " 'finality': 33852,\n",
       " 'rareness': 57436,\n",
       " 'stockinged': 82941,\n",
       " 'moniker': 20064,\n",
       " 'durrell': 82101,\n",
       " \"'english\": 41134,\n",
       " \"document's\": 73016,\n",
       " \"'24\": 87592,\n",
       " 'stechino': 71456,\n",
       " 'conquerer': 43155,\n",
       " 'novel': 664,\n",
       " \"'hare\": 33845,\n",
       " 'baddie': 8538,\n",
       " 'dubliner': 57686,\n",
       " 'aesthetics': 15750,\n",
       " 'meg': 5796,\n",
       " 'flyers': 71540,\n",
       " 'curates': 76511,\n",
       " 'montreux': 74544,\n",
       " 'embraced': 14662,\n",
       " 'ambersoms': 64199,\n",
       " 'livinston': 62632,\n",
       " 'challenged': 5232,\n",
       " 'protector': 12033,\n",
       " 'risdon': 85919,\n",
       " 'metamorphosis': 12651,\n",
       " 'injure': 42647,\n",
       " 'stoning': 34811,\n",
       " 'tonalities': 27210,\n",
       " 'dessicated': 46277,\n",
       " 'symbiote': 51974,\n",
       " 'viejo': 64165,\n",
       " 'ringu': 15047,\n",
       " 'hitlerism': 63223,\n",
       " 'cheyenne': 16274,\n",
       " \"'bleedmedry\": 52363,\n",
       " 'concurrent': 43980,\n",
       " 'mountains': 3984,\n",
       " \"sunny's\": 44015,\n",
       " 'deportivo': 77268,\n",
       " 'enchants': 42493,\n",
       " 'depending': 5594,\n",
       " 'swings\\x97but': 88067,\n",
       " 'malarky': 43260,\n",
       " 'inauthentic': 24018,\n",
       " 'latex': 17322,\n",
       " 'brightly': 12892,\n",
       " 'inquisitor': 49087,\n",
       " 'kurylenko': 45207,\n",
       " 'lingered': 18240,\n",
       " 'knuckled': 47440,\n",
       " 'sperm': 16766,\n",
       " \"putnam's\": 54802,\n",
       " 'invisible': 2501,\n",
       " \"videotape'\": 83207,\n",
       " 'holograms': 38788,\n",
       " 'hopi': 57228,\n",
       " 'brigand': 81390,\n",
       " 'fakest': 57835,\n",
       " \"dam's\": 80811,\n",
       " 'requiem': 17136,\n",
       " 'garafalo': 45324,\n",
       " 'beckinsale': 7464,\n",
       " 'starfleet': 15528,\n",
       " 'zizte': 66655,\n",
       " 'tripp': 30641,\n",
       " 'cuts': 1899,\n",
       " 'ultramagnetic': 83640,\n",
       " 'kents': 73065,\n",
       " 'ra': 13625,\n",
       " 'synopsize': 86310,\n",
       " 'abyssmal': 41617,\n",
       " 'transatlantic': 41880,\n",
       " 'sullen': 13539,\n",
       " 'headsets': 61386,\n",
       " 'whinnying': 81228,\n",
       " 'wrecked': 16158,\n",
       " 'neuroticism': 66707,\n",
       " 'drifted': 20850,\n",
       " 'christophe': 24533,\n",
       " 'trailer': 1469,\n",
       " 'sisterhood': 38799,\n",
       " \"beth's\": 36013,\n",
       " 'cuffs': 46674,\n",
       " 'english': 628,\n",
       " 'vérité': 54087,\n",
       " 'disband': 66502,\n",
       " 'earphone': 69489,\n",
       " \"ulysses'\": 86615,\n",
       " 'inconveniences': 64082,\n",
       " 'circumnavigate': 67247,\n",
       " \"henson's\": 29123,\n",
       " 'salvages': 30806,\n",
       " 'judaism': 26611,\n",
       " 'mckay': 17163,\n",
       " 'chickenpox': 53854,\n",
       " 'harpy': 26123,\n",
       " 'basic': 1118,\n",
       " 'gillman': 58852,\n",
       " \"statue's\": 68983,\n",
       " 'ladders': 34104,\n",
       " \"'quality\": 55348,\n",
       " 'firehouse': 25229,\n",
       " 'urghh': 58713,\n",
       " 'afb': 43112,\n",
       " \"devos'\": 50185,\n",
       " 'buff': 4298,\n",
       " 'psycopaths': 80727,\n",
       " 'nicolosi': 63585,\n",
       " 'gallery': 7233,\n",
       " 'nix': 85150,\n",
       " 'bye': 5455,\n",
       " 'winslet': 11271,\n",
       " 'ramgarh': 52508,\n",
       " 'enoch': 32498,\n",
       " \"webster's\": 39812,\n",
       " 'calamine': 45232,\n",
       " 'refuges': 59179,\n",
       " \"herself'\": 60619,\n",
       " 'poliwhirl': 86652,\n",
       " 'degenerating': 68480,\n",
       " 'anarchistic': 62507,\n",
       " '6k': 44773,\n",
       " 'hoffer': 64695,\n",
       " \"rye's\": 67990,\n",
       " 'limousine': 24794,\n",
       " 'assuaged': 65137,\n",
       " \"'too'\": 63864,\n",
       " 'pleasantness': 55036,\n",
       " 'shimmy': 84049,\n",
       " 'hairpieces': 70926,\n",
       " 'montford': 22151,\n",
       " 'hupfel': 61204,\n",
       " 'plebeians': 69112,\n",
       " \"orwell's\": 21161,\n",
       " 'lawaris': 79824,\n",
       " 'disagreeable': 33139,\n",
       " \"goodman's\": 18784,\n",
       " 'preventable': 87461,\n",
       " 'misfortunate': 55165,\n",
       " 'smuggling': 10206,\n",
       " 'prospers': 70353,\n",
       " 'violet': 14357,\n",
       " 'fishnet': 62881,\n",
       " 'brandy': 22918,\n",
       " 'pseudonyms': 66114,\n",
       " \"analyst's\": 55166,\n",
       " 'illegal': 4664,\n",
       " 'daniele': 58500,\n",
       " 'detriments': 69425,\n",
       " 'utilities\\x85': 58004,\n",
       " 'reservation': 14799,\n",
       " 'trudging': 38816,\n",
       " 'strikingly': 9543,\n",
       " 'nopes': 55113,\n",
       " 'anc': 33535,\n",
       " \"'adventures\": 69484,\n",
       " 'rhytmic': 83723,\n",
       " 'printing': 50624,\n",
       " 'exagerated': 61934,\n",
       " 'leão': 61241,\n",
       " 'multi': 3282,\n",
       " \"cgi'd\": 40784,\n",
       " 'interweave': 40270,\n",
       " 'parkas': 85623,\n",
       " 'hitchiker': 85109,\n",
       " 'pollutions': 57888,\n",
       " 'justice\\x85': 68750,\n",
       " 'policier': 49266,\n",
       " 'oskorblyonnye': 87254,\n",
       " 'pursuing': 7983,\n",
       " \"wish'd\": 59783,\n",
       " 'bending': 13116,\n",
       " 'asher': 30061,\n",
       " 'mmiv': 52947,\n",
       " 'emergencies': 37240,\n",
       " \"nazis'\": 41626,\n",
       " 'underlined': 23531,\n",
       " 'frannie': 46067,\n",
       " 'implies': 7803,\n",
       " 'daniel': 2271,\n",
       " 'formalism': 57072,\n",
       " 'congenial': 46290,\n",
       " \"morales'\": 86910,\n",
       " 'imperialflags': 63328,\n",
       " 'age\\x97kudos': 64291,\n",
       " \"crazy's\": 53281,\n",
       " 'panhandle': 31308,\n",
       " 'manned': 26812,\n",
       " \"jnr'\": 35763,\n",
       " 'indie': 2686,\n",
       " 'liang': 15212,\n",
       " 'psychotherapy': 79545,\n",
       " 'unleashing': 19225,\n",
       " 'ornella': 51999,\n",
       " 'fricker': 11089,\n",
       " 'jeopardized': 57561,\n",
       " \"proof'\": 67862,\n",
       " 'bertha': 67744,\n",
       " 'carbines': 43761,\n",
       " 'bloodwaters': 52786,\n",
       " 'pardon': 11690,\n",
       " \"journey's\": 32530,\n",
       " 'posture': 21087,\n",
       " \"warden's\": 63460,\n",
       " 'ginelli': 77508,\n",
       " 'zuni': 43380,\n",
       " 'chace': 30134,\n",
       " 'saleen': 48329,\n",
       " 'locomotive': 46945,\n",
       " 'agnes': 13255,\n",
       " 'bath': 4545,\n",
       " 'transformational': 63101,\n",
       " 'relaying': 85289,\n",
       " 'subscribed': 31004,\n",
       " 'brandie': 64967,\n",
       " 'velizar': 59290,\n",
       " 'biroc': 68947,\n",
       " 'nastasya': 36482,\n",
       " 'beack': 62081,\n",
       " 'hugh': 3931,\n",
       " \"'beast'\": 46733,\n",
       " \"anouska's\": 64856,\n",
       " 'solve': 3327,\n",
       " 'reclaimed': 41029,\n",
       " 'pensive': 26325,\n",
       " 'tasting': 20618,\n",
       " \"shawn's\": 68013,\n",
       " 'fetishists': 32816,\n",
       " \"d'amato\": 15532,\n",
       " 'goth': 11135,\n",
       " 'brewer': 46152,\n",
       " 'nonpolitical': 71251,\n",
       " 'galaxy': 6398,\n",
       " \"pressburger's\": 30435,\n",
       " 'oversold': 45300,\n",
       " 'crossings': 62399,\n",
       " 'lemondrop': 72650,\n",
       " 'gozu': 32369,\n",
       " 'afficionados': 45303,\n",
       " 'mcg': 77102,\n",
       " 'surprising': 1764,\n",
       " 'goners': 76538,\n",
       " 'bennie': 87826,\n",
       " 'thingie': 37239,\n",
       " \"loomis'\": 75515,\n",
       " 'vepsaian': 74894,\n",
       " 'tyranny': 15202,\n",
       " 'naaah': 77394,\n",
       " \"dunsky's\": 62040,\n",
       " 'characters': 102,\n",
       " 'mansfield': 11159,\n",
       " 'harpsichord': 34789,\n",
       " 'loses': 1975,\n",
       " 'tarlow': 84874,\n",
       " 'bfi': 78680,\n",
       " 'midproduction': 72954,\n",
       " 'rejecting': 17942,\n",
       " 'heartache': 21820,\n",
       " 'schnook': 40470,\n",
       " 'tarpon': 66920,\n",
       " 'integrity': 5036,\n",
       " 'upheaval': 18479,\n",
       " 'shines': 3141,\n",
       " 'greying': 75454,\n",
       " 'noodles': 44656,\n",
       " 'rebellious': 6841,\n",
       " \"'santa\": 69693,\n",
       " 'whooo': 41765,\n",
       " 'ostfront': 41217,\n",
       " \"hetfield's\": 52531,\n",
       " 'debase': 51283,\n",
       " 'elinore': 16061,\n",
       " \"sequel'\": 86538,\n",
       " 'dhoom2': 55354,\n",
       " 'dillemma': 52809,\n",
       " 'minces': 41543,\n",
       " 'echt': 44935,\n",
       " 'patrols': 38228,\n",
       " 'ewa': 40558,\n",
       " 'ortelli': 84604,\n",
       " 'mori': 52577,\n",
       " 'hammily': 86086,\n",
       " 'amplifying': 58605,\n",
       " 'borrower': 55368,\n",
       " 'kosti': 82033,\n",
       " 'businessman': 5021,\n",
       " 'chevette': 71509,\n",
       " 'fraudulently': 81398,\n",
       " 'paramilitaries': 28552,\n",
       " \"'haunting\": 58204,\n",
       " 'jenson': 42404,\n",
       " 'egyptology': 47877,\n",
       " 'clemens': 27531,\n",
       " 'archery': 32323,\n",
       " 'cheapens': 44180,\n",
       " 'entirely': 1094,\n",
       " 'drilling': 18042,\n",
       " 'dishing': 23278,\n",
       " 'swishing': 35964,\n",
       " \"hand's\": 51203,\n",
       " 'logically': 13333,\n",
       " 'tactics': 6918,\n",
       " 'hound': 12400,\n",
       " 'ambersons': 24813,\n",
       " 'spinsterhood': 73922,\n",
       " 'atmospherically': 30793,\n",
       " 'cochrane': 56502,\n",
       " 'wondrously': 42220,\n",
       " 'watt': 85647,\n",
       " \"tukur's\": 62434,\n",
       " 'identifiable': 16712,\n",
       " 'moneyed': 48151,\n",
       " 'vacation': 3032,\n",
       " 'inactivity': 86185,\n",
       " 'sheet': 9685,\n",
       " 'verson': 59248,\n",
       " 'casualties': 18733,\n",
       " 'similarities': 4348,\n",
       " 'princess': 2546,\n",
       " 'vandeuvres': 47710,\n",
       " 'inquirer': 71021,\n",
       " 'gruelingly': 60465,\n",
       " 'goldman': 36118,\n",
       " \"mumbai's\": 48178,\n",
       " 'expressway': 67649,\n",
       " 'sandbox': 35058,\n",
       " \"'viva\": 58803,\n",
       " 'godzirra': 70588,\n",
       " 'whizz': 72909,\n",
       " \"'date'\": 69171,\n",
       " \"rajinikanth's\": 41737,\n",
       " 'foliés': 64404,\n",
       " 'bhand': 54048,\n",
       " 'leer': 29838,\n",
       " 'purr': 48565,\n",
       " 'spiritualist': 33674,\n",
       " 'klown': 47916,\n",
       " \"\\x91order'\": 76214,\n",
       " 'castlevania': 84487,\n",
       " 'fannn': 72480,\n",
       " \"'los\": 53658,\n",
       " 'jobber': 75513,\n",
       " \"gays'\": 78820,\n",
       " 'distributer': 50036,\n",
       " 'chomet': 26089,\n",
       " 'vita': 44631,\n",
       " 'slinking': 32016,\n",
       " \"'achcha\": 87740,\n",
       " 'hawas': 67159,\n",
       " \"plantation's\": 50332,\n",
       " 'degenerate': 13397,\n",
       " 'limaye': 42363,\n",
       " 'katana': 26542,\n",
       " 'simon': 2179,\n",
       " 'native': 2171,\n",
       " 'backorder': 54290,\n",
       " \"weww's\": 63393,\n",
       " 'aah': 79765,\n",
       " 'zira': 37958,\n",
       " 'fictional': 2612,\n",
       " \"kylie's\": 88469,\n",
       " 'almighty': 8671,\n",
       " 'dogmatists': 68636,\n",
       " 'janie': 38923,\n",
       " 'sooooooo': 43005,\n",
       " 'fragmentation': 53052,\n",
       " 'unbelievers': 36985,\n",
       " 'krutcher': 78156,\n",
       " 'harnesses': 46681,\n",
       " 'oppositions': 48687,\n",
       " 'deewar': 43372,\n",
       " 'bandaras': 54593,\n",
       " 'olivera': 46111,\n",
       " 'fata': 42797,\n",
       " 'calomari': 85974,\n",
       " 'adrift': 20658,\n",
       " 'hires': 6233,\n",
       " 'rochesters': 46084,\n",
       " 'accession': 39731,\n",
       " 'urchin': 32216,\n",
       " 'misforgivings': 73359,\n",
       " 'bbc1': 24396,\n",
       " \"souza's\": 87890,\n",
       " 'tehrani': 52751,\n",
       " 'apprehensive': 26273,\n",
       " 'etch': 42052,\n",
       " 'enmeshed': 23438,\n",
       " 'roger': 2474,\n",
       " 'hhe2': 40657,\n",
       " 'fox': 1672,\n",
       " 'intercuts': 33493,\n",
       " '1000lb': 79728,\n",
       " 'trilateralists': 54023,\n",
       " \"kosleck's\": 47138,\n",
       " 'expressing': 9252,\n",
       " 'maxwells': 83305,\n",
       " 'marking': 12439,\n",
       " 'corsaire': 35107,\n",
       " 'untried': 43254,\n",
       " 'terrace': 20495,\n",
       " \"'trip'\": 81148,\n",
       " 'whammy': 25990,\n",
       " 'flower': 7008,\n",
       " 'unawareness': 84683,\n",
       " 'summary': 2732,\n",
       " 'choreographic': 47956,\n",
       " 'rampaging': 21696,\n",
       " 'entities': 30690,\n",
       " 'stoically': 37789,\n",
       " 'lopez': 7603,\n",
       " 'earthling': 32296,\n",
       " 'feebly': 31259,\n",
       " 'pancakes': 19433,\n",
       " 'martinaud': 43462,\n",
       " 'shhhhh': 68065,\n",
       " 'brazilians': 51510,\n",
       " \"1987's\": 70536,\n",
       " 'punked': 72039,\n",
       " 'canoeists': 49894,\n",
       " 'kraakman': 39568,\n",
       " 'automakers': 27829,\n",
       " 'adolescent': 5591,\n",
       " 'constantine': 22269,\n",
       " 'dominion': 23173,\n",
       " 'hatsumomo': 68781,\n",
       " 'unengaged': 51834,\n",
       " 'raunchily': 47804,\n",
       " 'unmotivated': 13591,\n",
       " 'janowski': 55013,\n",
       " 'indochine': 30863,\n",
       " 'yancey': 62138,\n",
       " 'kink': 23581,\n",
       " \"'animal\": 76646,\n",
       " 'avro': 85714,\n",
       " \"'menagerie'\": 55569,\n",
       " 'untertones': 87863,\n",
       " 'schlessinger': 52203,\n",
       " 'paltrow': 4828,\n",
       " 'nicely': 1777,\n",
       " 'eurocult': 54694,\n",
       " 'removing': 12340,\n",
       " 'lettering': 54306,\n",
       " 'aww': 45644,\n",
       " \"fleming's\": 30859,\n",
       " 'fraser': 16585,\n",
       " \"blandings'\": 28115,\n",
       " 'stroke': 6644,\n",
       " 'napoli': 28445,\n",
       " 'definate': 48310,\n",
       " 'disowns': 85269,\n",
       " 'vindication': 32570,\n",
       " 'christianity': 5278,\n",
       " 'donated': 18600,\n",
       " \"'sleeper'\": 55426,\n",
       " 'sunglass': 68175,\n",
       " 'extravagances': 54597,\n",
       " \"mormon's\": 52225,\n",
       " 'mdogg20': 73453,\n",
       " 'civvies': 71565,\n",
       " 'cotta': 42074,\n",
       " 'unfortunatly': 34320,\n",
       " \"tourneur's\": 28166,\n",
       " 'crackling': 18669,\n",
       " \"seaver's\": 66662,\n",
       " 'untrammelled': 78228,\n",
       " 'electromagnetic': 49887,\n",
       " 'lawston': 45800,\n",
       " \"reality's\": 81650,\n",
       " 'savour': 24994,\n",
       " 'attributions': 79194,\n",
       " 'tins': 47153,\n",
       " 'zering': 63959,\n",
       " 'in': 8,\n",
       " 'rightous': 61339,\n",
       " 'hartnett': 14599,\n",
       " \"'race\": 39940,\n",
       " 'touchdown': 30991,\n",
       " 'caballe': 86320,\n",
       " \"3po's\": 57696,\n",
       " 'delli': 64526,\n",
       " 'tepidly': 86197,\n",
       " \"kasdan's\": 36589,\n",
       " 'talkies': 12017,\n",
       " 'pah': 23314,\n",
       " 'denomination': 46317,\n",
       " 'fect': 60847,\n",
       " 'contents': 11042,\n",
       " 'washroom': 38818,\n",
       " 'airsoft': 88573,\n",
       " 'hokum': 14289,\n",
       " 'themsleves': 74569,\n",
       " 'olde': 30661,\n",
       " 'floberg': 65545,\n",
       " 'adjusts': 40556,\n",
       " 'come': 213,\n",
       " 'aya': 22378,\n",
       " 'pigalle': 29885,\n",
       " 'parton': 34470,\n",
       " 'pecan': 86169,\n",
       " 'mcarthur': 65388,\n",
       " 'gunfighting': 32765,\n",
       " 'chu': 13666,\n",
       " 'backwards': 5887,\n",
       " 'fogelman': 59885,\n",
       " 'profitability': 87401,\n",
       " 'finerman': 42669,\n",
       " 'containment': 34895,\n",
       " 'goodtime': 81590,\n",
       " 'chrissakes': 33089,\n",
       " 'expressed': 4974,\n",
       " 'barroom': 32924,\n",
       " 'possessions': 19453,\n",
       " 'wembley': 24934,\n",
       " 'eyepatch': 56955,\n",
       " \"close's\": 48750,\n",
       " \"zach's\": 67854,\n",
       " 'earthiness': 79155,\n",
       " 'duuh': 71824,\n",
       " 'tutu': 51447,\n",
       " \"prisoner'\": 85205,\n",
       " 'dohhh': 65825,\n",
       " 'indefatigable': 36196,\n",
       " 'imitates': 19980,\n",
       " ...}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 번호와 단어의 관계를 사전으로 만든다. 1번은 문장의 시작, 2번은 사전에 없는 단어(Out of Vocabulary)로 미리 지정되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word = {idx+3: word for word, idx in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word[1] = '<START>'\n",
    "index_word[2] = '<UNKNOWN>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 번호로 된 데이터를 단어로 변환해 본다. 실제 데이터 분석에서는 단어로 된 데이터를 단어 번호로 변환해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(index_word[i] for i in x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 총 갯수를 변수에 할당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_WORDS = max(index_word) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 텍스트를 단어 번호로 바꾸기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 단어 번호로 바꾸는 방법을 알아보기 위해 먼저 데이터를 텍스트로 역변환하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "for data in x_train:\n",
    "    text = ' '.join(index_word[i] for i in data)\n",
    "    texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> b movie at best sound effects are pretty good lame concept decent execution i suppose it's a rental br br you put some olive oil in your mouth to save you from de poison den you cut de bite and suck out de poisen you gonna be ok tommy br br you stay by the airphone when agent harris calls you get me give me a fire extinguisher br br weapons we need weapons where's the silverware all we have is this sporks br br dr price is the snake expert br br local ers can handle the occasional snakebite alert every er in the tri city area\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 단어번호로 바꾸는 것은 Tokenizer를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok = Tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_on_texts를 이용해 texts에 있는 단어들에 번호를 매긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tok.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts_to_sequences를 이용해 텍스트를 실제로 단어 번호 리스트로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_data = tok.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28,\n",
       " 11,\n",
       " 19,\n",
       " 13,\n",
       " 41,\n",
       " 526,\n",
       " 968,\n",
       " 1618,\n",
       " 1381,\n",
       " 63,\n",
       " 455,\n",
       " 4449,\n",
       " 64,\n",
       " 3930,\n",
       " 1,\n",
       " 171,\n",
       " 34,\n",
       " 254,\n",
       " 2,\n",
       " 22,\n",
       " 98,\n",
       " 41,\n",
       " 835,\n",
       " 110,\n",
       " 48,\n",
       " 667,\n",
       " 21905,\n",
       " 6,\n",
       " 33,\n",
       " 477,\n",
       " 282,\n",
       " 2,\n",
       " 148,\n",
       " 1,\n",
       " 170,\n",
       " 110,\n",
       " 165,\n",
       " 20598,\n",
       " 334,\n",
       " 382,\n",
       " 37,\n",
       " 1,\n",
       " 170,\n",
       " 4517,\n",
       " 1105,\n",
       " 14,\n",
       " 543,\n",
       " 36,\n",
       " 10,\n",
       " 444,\n",
       " 1,\n",
       " 190,\n",
       " 48,\n",
       " 13,\n",
       " 3,\n",
       " 145,\n",
       " 2016,\n",
       " 16,\n",
       " 11,\n",
       " 19,\n",
       " 1,\n",
       " 1906,\n",
       " 4588,\n",
       " 466,\n",
       " 1,\n",
       " 19,\n",
       " 69,\n",
       " 85,\n",
       " 9,\n",
       " 13,\n",
       " 41,\n",
       " 526,\n",
       " 36,\n",
       " 74,\n",
       " 12,\n",
       " 10,\n",
       " 1244,\n",
       " 1,\n",
       " 19,\n",
       " 14,\n",
       " 512,\n",
       " 14,\n",
       " 9,\n",
       " 13,\n",
       " 622,\n",
       " 15,\n",
       " 18510,\n",
       " 2,\n",
       " 60,\n",
       " 383,\n",
       " 9,\n",
       " 5,\n",
       " 314,\n",
       " 5,\n",
       " 104,\n",
       " 2,\n",
       " 1,\n",
       " 2218,\n",
       " 5229,\n",
       " 13,\n",
       " 477,\n",
       " 64,\n",
       " 3765,\n",
       " 31,\n",
       " 1,\n",
       " 128,\n",
       " 9,\n",
       " 13,\n",
       " 36,\n",
       " 614,\n",
       " 2,\n",
       " 22,\n",
       " 122,\n",
       " 49,\n",
       " 34,\n",
       " 133,\n",
       " 45,\n",
       " 22,\n",
       " 1408,\n",
       " 31,\n",
       " 3,\n",
       " 19,\n",
       " 9,\n",
       " 213,\n",
       " 25,\n",
       " 75,\n",
       " 50,\n",
       " 2,\n",
       " 11,\n",
       " 404,\n",
       " 13,\n",
       " 80,\n",
       " 10307,\n",
       " 5,\n",
       " 1,\n",
       " 105,\n",
       " 115,\n",
       " 5924,\n",
       " 12,\n",
       " 254,\n",
       " 1,\n",
       " 30568,\n",
       " 4,\n",
       " 3734,\n",
       " 2,\n",
       " 720,\n",
       " 34,\n",
       " 69,\n",
       " 41,\n",
       " 526,\n",
       " 473,\n",
       " 23,\n",
       " 396,\n",
       " 315,\n",
       " 44,\n",
       " 4,\n",
       " 1,\n",
       " 11926,\n",
       " 1025,\n",
       " 10,\n",
       " 102,\n",
       " 86,\n",
       " 1,\n",
       " 378,\n",
       " 12,\n",
       " 295,\n",
       " 96,\n",
       " 30,\n",
       " 2064,\n",
       " 54,\n",
       " 23,\n",
       " 139,\n",
       " 3,\n",
       " 192,\n",
       " 7476,\n",
       " 15,\n",
       " 1,\n",
       " 224,\n",
       " 19,\n",
       " 18,\n",
       " 132,\n",
       " 473,\n",
       " 23,\n",
       " 477,\n",
       " 2,\n",
       " 142,\n",
       " 27,\n",
       " 5514,\n",
       " 15,\n",
       " 49,\n",
       " 34,\n",
       " 25,\n",
       " 222,\n",
       " 90,\n",
       " 22,\n",
       " 102,\n",
       " 1,\n",
       " 224,\n",
       " 63,\n",
       " 13,\n",
       " 36,\n",
       " 1331,\n",
       " 86,\n",
       " 9,\n",
       " 13,\n",
       " 281,\n",
       " 2,\n",
       " 13,\n",
       " 4450,\n",
       " 111,\n",
       " 101,\n",
       " 30,\n",
       " 12,\n",
       " 13,\n",
       " 5340,\n",
       " 16,\n",
       " 176,\n",
       " 30]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28, 11, 19, 13, 41, 526, 968, 1618, 1381, 63]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 대상단어와 맥락단어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec은 대상 단어와 맥락 단어(대상 단어 주변의 단어)의 관계를 학습시키는 방법이다. 한 단어씩 옮겨가며 좌우로 주변 2단어를 맥락 단어에 포함시키는 함수를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_target_context(x):\n",
    "    targets = []  # 대상단어\n",
    "    contexts = [] # 맥락단어들\n",
    "    for paragraph in x:\n",
    "        n = len(paragraph)\n",
    "        window = 2  # 좌우로 각 2단어씩\n",
    "        for i in range(window, n-window):\n",
    "            # 대상단어\n",
    "            targets.append(paragraph[i])\n",
    "\n",
    "            # 맥락단어\n",
    "            contexts.append(\n",
    "                paragraph[i-window:i] +    # 왼쪽 맥락단어\n",
    "                paragraph[i+1:i+window+1]) # 오른쪽 맥락단어\n",
    "    return targets, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_train, context_train = extract_target_context(x_train)\n",
    "target_test, context_test = extract_target_context(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터에서 첫 5단어는 아래와 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대상단어는 가운데 22번이고,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맥락단어는 왼쪽 1, 14번과 오른쪽 16, 43번이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 16, 43]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 데이터 생성자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 신경망에 학습시키려면 행렬 형태로 바꿔야 한다. 그러나 데이터가 매우 많기 때문에 한 번에 행렬로 바꾸면 학습시키기가 어렵다. 그래서 생성자라는 형태로 만들어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import Sequence, to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras는 Sequence 클래스를 상속하는 형태로 데이터 생성자를 만들 수 있도록 하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TargetContext(Sequence):\n",
    "    def __init__(self, target, context, batch_size):\n",
    "        self.target = numpy.asarray(target)\n",
    "        self.context = numpy.asarray(context)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"데이터의 길이\"\"\"\n",
    "        # return len(self.context) // self.batch_size\n",
    "        return 128  # 간단히 하기 위해 128로 고정\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"idx 번째 데이터 배치를 가져온다\"\"\"\n",
    "        i = numpy.random.choice(len(self.target), self.batch_size)  # 실제로는 무작위로 batch_size만큼 데이터를 고른다\n",
    "        batch_x = self.context[i]\n",
    "        batch_y = self.target[i]\n",
    "        return batch_x, to_categorical(batch_y, NUM_WORDS)  # to_categorical로 y를 one-hot encoding을 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = TargetContext(target_train, context_train, 32)\n",
    "valid = TargetContext(target_test, context_test, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[24554,   419,     4,  1709],\n",
       "        [   33,     6,    16,   691],\n",
       "        [   73,  1391,    26,    46],\n",
       "        [  109,    24,   284,    19],\n",
       "        [   27,   118,  3622,     9],\n",
       "        [    8,    30,    23,    11],\n",
       "        [  117, 25656,    28,     4],\n",
       "        [  248,     9,  9825,    18],\n",
       "        [   11,     4,    16,   184],\n",
       "        [  316,   574,   874,   188],\n",
       "        [11649,  1824,     5,   472],\n",
       "        [  433,   225,    60,  2602],\n",
       "        [    9, 14822,   259,    15],\n",
       "        [  675,     7,    18,   373],\n",
       "        [   15,   123,  2339,     9],\n",
       "        [ 1835,    44,  3582,     8],\n",
       "        [   12,    86,    46,    11],\n",
       "        [  354,   533,   468,   230],\n",
       "        [   89,   966,   183,    26],\n",
       "        [    4,   402,    95,    25],\n",
       "        [16749,   180,    27,   154],\n",
       "        [   23,     4,    73,   917],\n",
       "        [   18,     6,     7,  1711],\n",
       "        [    8,  2061,   105,    60],\n",
       "        [    4,  2113,   193,   129],\n",
       "        [ 1981,   554,  1451,    42],\n",
       "        [  462,    99,    13,   828],\n",
       "        [ 2440,     7,  6919,     5],\n",
       "        [  434,     9,   345,    14],\n",
       "        [ 1212,    39,   621,  1971],\n",
       "        [   24,   773, 10497,  9626],\n",
       "        [   13,   202,   123,  1086]]),\n",
       " array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ..., \n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. NNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 레이어는 저장을 위해 따로 변수로 지정해둔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_nnlm = Embedding(input_dim=NUM_WORDS, output_dim=8, input_length=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnlm = Sequential()\n",
    "nnlm.add(emb_nnlm)\n",
    "nnlm.add(Flatten())\n",
    "nnlm.add(Dense(128, activation='relu'))\n",
    "nnlm.add(Dense(NUM_WORDS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              708704    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4224      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 88588)             11427852  \n",
      "=================================================================\n",
      "Total params: 12,140,780\n",
      "Trainable params: 12,140,780\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nnlm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnlm.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 98s 764ms/step - loss: 10.2028 - acc: 0.0549 - val_loss: 8.0361 - val_acc: 0.0613\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 84s 658ms/step - loss: 7.8740 - acc: 0.0515 - val_loss: 7.5118 - val_acc: 0.0613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181f21c358>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnlm.fit_generator(\n",
    "    train,\n",
    "    epochs=30,\n",
    "    validation_data=valid,\n",
    "    callbacks=[EarlyStopping(monitor='val_acc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임베딩 레이어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.save('emb_nnlm.npy', emb_nnlm.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW는 NNLM에서 은닉층을 없애고 대신 임베딩을 단순히 평균낸 것을 사용한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_cbow = Embedding(input_dim=NUM_WORDS, output_dim=8, input_length=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(emb_cbow)\n",
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1)))  # 임베딩의 평균\n",
    "cbow.add(Dense(NUM_WORDS, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 4, 8)              708704    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 88588)             797292    \n",
      "=================================================================\n",
      "Total params: 1,505,996\n",
      "Trainable params: 1,505,996\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbow.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 67s 522ms/step - loss: 11.3306 - acc: 0.0483 - val_loss: 11.2292 - val_acc: 0.0557\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 64s 502ms/step - loss: 11.0438 - acc: 0.0532 - val_loss: 10.7611 - val_acc: 0.0603\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 65s 507ms/step - loss: 10.4337 - acc: 0.0547 - val_loss: 10.0463 - val_acc: 0.0596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1862e473c8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.fit_generator(\n",
    "    train,\n",
    "    epochs=30,\n",
    "    validation_data=valid,\n",
    "    callbacks=[EarlyStopping(monitor='val_acc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임베딩 레이어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.save('emb_cbow.npy', emb_cbow.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skipgram은 구현하기가 조금 까다롭다. 맥락단어를 직접 예측하는 대신 대상 단어와 맥락 단어가 함께 입력되면 1을 출력하고, 대상 단어와 맥락 외 단어가 함께 입력되면 0을 출력하는 형식으로 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramData(TargetContext):\n",
    "    def __getitem__(self, idx):\n",
    "        n = self.batch_size // 2\n",
    "        i = numpy.random.choice(len(self.target), n)\n",
    "        j = numpy.random.choice(4, n)\n",
    "\n",
    "        true_context = self.context[i, j]  # 실제 맥락\n",
    "        true_target = self.target[i]       # 실제 대상\n",
    "        true_y = numpy.ones(n)             # 1\n",
    "\n",
    "        # 무작위로 단어를 뽑아 가짜 맥락을 만든다\n",
    "        false_context = numpy.random.choice(NUM_WORDS, n)\n",
    "        false_y = numpy.zeros(n)                           # 0\n",
    "\n",
    "         # 실제 맥락과 가짜 맥락을 이어 붙인다\n",
    "        context = numpy.append(true_context, false_context)\n",
    "\n",
    "        # 실제 대상을 2배로 한다\n",
    "        target = numpy.append(true_target, true_target)\n",
    "\n",
    "        # 앞부분은 1, 뒷부분은 0\n",
    "        y = numpy.append(true_y, false_y)\n",
    "\n",
    "        return [context, target], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_skipgram = SkipGramData(target_train, context_train, 32)\n",
    "valid_skipgram = SkipGramData(target_test, context_test, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모형은 이제까지 사용한 Sequential 모형 대신 keras의 함수형 방식을 사용한다. 이 방식은 각 레이어를 일종의 함수처럼 쓰는 방법이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dot, Input, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 입력 레이어\n",
    "input_target = Input(shape=(1,))\n",
    "input_context = Input(shape=(1,))\n",
    "\n",
    "# 임베딩 레이어\n",
    "emb_skip_target = Embedding(input_dim=NUM_WORDS, output_dim=8)\n",
    "emb_skip_context = Embedding(input_dim=NUM_WORDS, output_dim=8)\n",
    "\n",
    "# 입력 레이어를 임베딩 레이어에 연결시키고\n",
    "# 양쪽 임베딩 레이어를 Dot 레이어에 연결시킨다\n",
    "# Dot 레이어는 양쪽 입력을 곱하는 역할을 한다\n",
    "out = Dot(axes=2)([\n",
    "    emb_skip_target(input_target),\n",
    "    emb_skip_context(input_context)])\n",
    "\n",
    "# 출력 형태를 (1, 1)에서 (1,)로 바꾼다\n",
    "out = Reshape((1,), input_shape=(1, 1))(out)\n",
    "\n",
    "# 시그모이드로 출력한다\n",
    "out = Activation('sigmoid')(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skipgram = Model(inputs=[input_target, input_context], outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 8)         708704      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 8)         708704      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 1)         0           embedding_3[0][0]                \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1)            0           dot_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 1)            0           reshape_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,417,408\n",
      "Trainable params: 1,417,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "skipgram.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skipgram.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "128/128 [==============================] - 4s 31ms/step - loss: 0.6931 - acc: 0.5146 - val_loss: 0.6931 - val_acc: 0.5215\n",
      "Epoch 2/30\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.6930 - acc: 0.5193 - val_loss: 0.6928 - val_acc: 0.5225\n",
      "Epoch 3/30\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.6924 - acc: 0.5459 - val_loss: 0.6919 - val_acc: 0.5627\n",
      "Epoch 4/30\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.6910 - acc: 0.5725 - val_loss: 0.6900 - val_acc: 0.5881\n",
      "Epoch 5/30\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.6877 - acc: 0.6062 - val_loss: 0.6855 - val_acc: 0.6089\n",
      "Epoch 6/30\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.6826 - acc: 0.6238 - val_loss: 0.6776 - val_acc: 0.6282\n",
      "Epoch 7/30\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.6740 - acc: 0.6458 - val_loss: 0.6688 - val_acc: 0.6348\n",
      "Epoch 8/30\n",
      "128/128 [==============================] - 3s 25ms/step - loss: 0.6661 - acc: 0.6545 - val_loss: 0.6582 - val_acc: 0.6489\n",
      "Epoch 9/30\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.6543 - acc: 0.6682 - val_loss: 0.6488 - val_acc: 0.6631\n",
      "Epoch 10/30\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.6431 - acc: 0.6606 - val_loss: 0.6317 - val_acc: 0.6799\n",
      "Epoch 11/30\n",
      "128/128 [==============================] - 3s 25ms/step - loss: 0.6328 - acc: 0.6719 - val_loss: 0.6288 - val_acc: 0.6660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x186393ed68>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgram.fit_generator(\n",
    "    train_skipgram,\n",
    "    epochs=30,\n",
    "    validation_data=valid_skipgram,\n",
    "    callbacks=[EarlyStopping(monitor='val_acc')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임베딩 레이어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numpy.save('emb_skip_target.npy', emb_skip_target.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 임베딩 레이어 재사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "워드 임베딩을 학습시키는 이유는 그 자체로 목적이 있다기보다 다른 학습에 이를 재사용하여 학습 효율을 높이기 위해서다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAXLEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장한 레이어의 가중치를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = numpy.load('emb_skip_target.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩 레이어를 만든다. 아래에서 trainable=False로 하면 임베딩 레이어는 추가 학습을 하지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_ff = Embedding(input_dim=NUM_WORDS, output_dim=8, input_length=MAXLEN,\n",
    "                   weights=[w],   # 레이어 가중치를 저장한 값으로 설정한다\n",
    "                   trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 앞먹임 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN 수업에서 사용했던 앞먹임 신경망을 다시 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_seq = pad_sequences(x_train, MAXLEN)\n",
    "x_test_seq = pad_sequences(x_test, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff = Sequential()\n",
    "ff.add(emb_ff)  # 미리 만들어진 임베딩 레이어를 사용한다.\n",
    "ff.add(Flatten())\n",
    "ff.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20, 8)             708704    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 708,865\n",
      "Trainable params: 708,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff.compile(optimizer=RMSprop(), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "20000/20000 [==============================] - 2s 99us/step - loss: 0.6952 - acc: 0.5158 - val_loss: 0.6903 - val_acc: 0.5216\n",
      "Epoch 2/30\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.6772 - acc: 0.5971 - val_loss: 0.6785 - val_acc: 0.5854\n",
      "Epoch 3/30\n",
      "20000/20000 [==============================] - 2s 83us/step - loss: 0.6529 - acc: 0.6838 - val_loss: 0.6571 - val_acc: 0.6466\n",
      "Epoch 4/30\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.6127 - acc: 0.7470 - val_loss: 0.6217 - val_acc: 0.6814\n",
      "Epoch 5/30\n",
      "20000/20000 [==============================] - 2s 88us/step - loss: 0.5595 - acc: 0.7779 - val_loss: 0.5808 - val_acc: 0.7116\n",
      "Epoch 6/30\n",
      "20000/20000 [==============================] - 2s 91us/step - loss: 0.5079 - acc: 0.7994 - val_loss: 0.5489 - val_acc: 0.7248\n",
      "Epoch 7/30\n",
      "20000/20000 [==============================] - 2s 87us/step - loss: 0.4642 - acc: 0.8164 - val_loss: 0.5263 - val_acc: 0.7352\n",
      "Epoch 8/30\n",
      "20000/20000 [==============================] - 2s 84us/step - loss: 0.4292 - acc: 0.8302 - val_loss: 0.5123 - val_acc: 0.7430\n",
      "Epoch 9/30\n",
      "20000/20000 [==============================] - 2s 83us/step - loss: 0.4009 - acc: 0.8420 - val_loss: 0.5039 - val_acc: 0.7456\n",
      "Epoch 10/30\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.3768 - acc: 0.8524 - val_loss: 0.4983 - val_acc: 0.7492\n",
      "Epoch 11/30\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.3554 - acc: 0.8623 - val_loss: 0.4967 - val_acc: 0.7486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x188212fc88>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff.fit(\n",
    "  x_train_seq,\n",
    "    y_train,\n",
    "    epochs=30,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[EarlyStopping(monitor='val_acc')])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
